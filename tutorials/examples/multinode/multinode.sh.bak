#!/bin/bash
#SBATCH --job-name=HYPERGRID_MULTINODE_TEST
#SBATCH --output=/home/mila/v/vivianoj/torchgfn/outputs/%x_output.txt
#SBATCH --error=/home/mila/v/vivianoj/torchgfn/outputs/%x_error.txt
#SBATCH --time=0-02:00
#SBATCH --partition=short-unkillable    # ask for unkillable job
#SBATCH --nodes=2                       # number of nodes
#SBATCH --switches=1@01:00:00           # number of leaf switches (InfiniBand Island) with time limit for the constraint
#SBATCH --ntasks-per-node=1             # only 1 task per node!
#SBATCH --gpus-per-task=0               # number of gpus per node
#SBATCH --cpus-per-task=12              # number of cpus per node
#SBATCH --mem=32G                       # memory per node
#SBATCH --prefer=x86_64                 # constraints: note accepts bools | &
#SBATCH --signal=TERM@60                # SIGTERM 60s prior to the allocation's end - autocheckpoint!

# Get a unique port for this job based on the job ID.
export MASTER_PORT=$(expr 10000 + $(echo -n $SLURM_JOBID | tail -c 4))
export MASTER_ADDR=$SLURMD_NODENAME  # $(scontrol show hostnames $SLURM_JOB_NODELIST | head -n 1)

# Required as --cpus-per-gpu is not supported by the short-unkillable.
# export SLURM_CPUS_PER_GPU=$(($SLURM_CPUS_PER_TASK / $SLURM_GPUS_PER_TASK))

# Maximum number of threads in the OpenMP parallel region (defaults to 1).
# (called by `torch.distributed.run`, called by `accelerate launch`)
export OMP_NUM_THREADS=1  #=$SLURM_CPUS_PER_TASK
#export OMP_NUM_THREADS=$SLURM_CPUS_PER_GPU

# Logging.
export TORCH_CPP_LOG_LEVEL="INFO"
export TORCH_DISTRIBUTED_DEBUG="INFO"
export CCL_LOG_LEVEL="debug"
export NCCL_DEBUG="INFO"

# Intel MPI Variables
export PMI_NUM=2
export PMI_SIZE=2

# Train.
eval "$(conda shell.bash hook)"
conda activate torchgfn
cd $HOME/code/torchgfn/tutorials/examples

# These are the correct commands when we add in management of GPUs.
# --cpus-per-gpu=$SLURM_CPUS_PER_GPU \
# --gpus-per-task=$SLURM_GPUS_PER_TASK \

# This command should get evaluated on each node, so the relevant variables
# will not be the same on each.
per_node_command=$(cat <<"EOF"
export PMI_RANK=$SLURM_JOBID
torchrun \
--nnodes=$SLURM_JOB_NUM_NODES \
--nproc-per-node=$SLURM_CPUS_PER_TASK \
--rdzv-id=$SLURM_JOBID \
--rdzv-backend=c10d \
--rdzv-endpoint=$MASTER_ADDR:$MASTER_PORT \
python train_hypergrid_multinode.py
EOF
)

# The variables here are going to be evaluated by the SLURM controller, not on
# each node.
srun \
    --kill-on-bad-exit=1 \
    --nodes=$SLURM_JOB_NUM_NODES \
    --ntasks=$SLURM_JOB_NUM_NODES \
    --cpus-per-task=$SLURM_CPUS_PER_TASK \
    --ntasks-per-node=1 \
    bash -c 'export PMI_RANK=$SLURM_JOBID; torchrun --nnodes=$SLURM_JOB_NUM_NODES --nproc-per-node=$SLURM_CPUS_PER_TASK --rdzv-id=$SLURM_JOBID --rdzv-backend=c10d --rdzv-endpoint=$MASTER_ADDR:$MASTER_PORT python train_hypergrid_multinode.py'

